"""
AEP MORALITY - EXPERIMENTAL PROTOCOLS
Implementation of validation protocols from Section 7
NUMPY-ONLY VERSION - No scipy or specialized dependencies
"""

import numpy as np
import math

class ExperimentalProtocols:
    """
    Implements experimental protocols for validating AEP morality hypotheses
    H1: Compression-Signature Hypothesis
    H2: Complexity-Tax Hypothesis  
    H3: Alignment-Dynamics Hypothesis
    """
    
    def __init__(self):
        self.protocol_results = {}
        
    def simulate_fmri_data(self, n_subjects=30, n_trials=100):
        """
        Protocol 1: fMRI of Honesty vs Deception
        Simulates neural compression metrics during moral decisions
        """
        print("PROTOCOL 1: fMRI HONESTY vs DECEPTION")
        print("=" * 50)
        
        # Generate synthetic fMRI data
        honest_trials = {
            'intrinsic_dimensionality': np.random.normal(18.3, 2.1, (n_subjects, n_trials)),
            'predictive_complexity': np.random.normal(0.124, 0.03, (n_subjects, n_trials)),
            'information_integration': np.random.normal(0.67, 0.08, (n_subjects, n_trials))
        }
        
        deceptive_trials = {
            'intrinsic_dimensionality': np.random.normal(23.7, 3.2, (n_subjects, n_trials)),
            'predictive_complexity': np.random.normal(0.158, 0.04, (n_subjects, n_trials)),
            'information_integration': np.random.normal(0.52, 0.09, (n_subjects, n_trials))
        }
        
        # Calculate group means and effects
        results = {}
        for metric in honest_trials.keys():
            honest_mean = np.mean(honest_trials[metric])
            deceptive_mean = np.mean(deceptive_trials[metric])
            effect_size = (deceptive_mean - honest_mean) / np.sqrt(
                (np.var(honest_trials[metric]) + np.var(deceptive_trials[metric])) / 2
            )
            
            # Simplified t-test
            t_stat = effect_size * math.sqrt(n_subjects / 2)
            p_value = 2 * (1 - self.t_cdf_approx(abs(t_stat), 2*n_subjects-2))
            
            results[metric] = {
                'honest_mean': honest_mean,
                'deceptive_mean': deceptive_mean,
                'effect_size': abs(effect_size),
                'p_value': p_value,
                'significant': p_value < 0.05
            }
            
            print(f"{metric:>25}: d = {effect_size:5.2f}, p = {p_value:6.4f}, "
                  f"{'SIGNIFICANT' if p_value < 0.05 else 'non-sig'}")
        
        self.protocol_results['fmri_honesty_deception'] = results
        return results
    
    def simulate_eeg_courage(self, n_subjects=25):
        """
        Protocol 2: EEG of Courageous Action
        Tests Alignment-Dynamics Hypothesis (H3)
        """
        print("\nPROTOCOL 2: EEG COURAGEOUS ACTION")
        print("=" * 50)
        
        # Simulate EEG features: LPP amplitude and frontal-parietal synchrony
        courageous_trials = {
            'lpp_amplitude': np.random.normal(8.2, 1.5, n_subjects),  # Larger LPP
            'synchrony': np.random.normal(0.68, 0.12, n_subjects),    # Higher synchrony
            'alignment_confidence': np.random.normal(0.75, 0.15, n_subjects)  # Higher Î¾
        }
        
        fearful_trials = {
            'lpp_amplitude': np.random.normal(5.1, 1.8, n_subjects),  # Smaller LPP  
            'synchrony': np.random.normal(0.42, 0.18, n_subjects),    # Lower synchrony
            'alignment_confidence': np.random.normal(0.35, 0.20, n_subjects)  # Lower Î¾
        }
        
        # Logistic regression simulation for intervention probability
        print("Logistic Regression: logit(P(Intervene)) ~ LPP + Synchrony + Î¾")
        
        # Simulate regression coefficients
        coefficients = {
            'lpp_amplitude': 0.35,
            'synchrony': 0.28, 
            'alignment_confidence': 0.42,
            'intercept': -2.1
        }
        
        # Calculate predicted probabilities
        courageous_prob = self.logistic_predict(courageous_trials, coefficients)
        fearful_prob = self.logistic_predict(fearful_trials, coefficients)
        
        print(f"Courageous trials: P(intervene) = {np.mean(courageous_prob):.3f}")
        print(f"Fearful trials: P(intervene) = {np.mean(fearful_prob):.3f}")
        print(f"Odds ratio: {np.mean(courageous_prob)/np.mean(fearful_prob):.2f}")
        
        # Test H3: Courage increases alignment confidence
        xi_correlation, xi_p_value = self.pearson_correlation(
            courageous_trials['alignment_confidence'],
            courageous_trials['synchrony']
        )
        
        print(f"\nH3 Validation: Î¾ ~ Synchrony correlation: r = {xi_correlation:.3f}, p = {xi_p_value:.4f}")
        if xi_correlation > 0.3:
            print("âœ“ Alignment-Dynamics Hypothesis SUPPORTED")
        else:
            print("âœ— Alignment-Dynamics Hypothesis not supported")
        
        results = {
            'courageous_prob': np.mean(courageous_prob),
            'fearful_prob': np.mean(fearful_prob),
            'xi_synchrony_correlation': xi_correlation,
            'xi_synchrony_p_value': xi_p_value
        }
        
        self.protocol_results['eeg_courage'] = results
        return results
    
    def simulate_organizational_complexity_tax(self, n_organizations=50):
        """
        Protocol 3: Complexity Tax in Organizations
        Tests Complexity-Tax Hypothesis (H2)
        """
        print("\nPROTOCOL 3: ORGANIZATIONAL COMPLEXITY TAX")
        print("=" * 50)
        
        # Generate organizational data
        relational_complexity = np.random.uniform(30, 100, n_organizations)
        
        # Complexity tax model: costs increase with relational complexity
        base_transaction_cost = 100000  # USD
        complexity_multiplier = 2500    # USD per complexity unit
        
        transaction_costs = base_transaction_cost + complexity_multiplier * relational_complexity
        compliance_costs = 0.7 * transaction_costs  # Correlated with transaction costs
        
        # Add noise
        transaction_costs += np.random.normal(0, 20000, n_organizations)
        compliance_costs += np.random.normal(0, 15000, n_organizations)
        
        total_costs = transaction_costs + compliance_costs
        
        # Regression analysis: Cost ~ Complexity + Controls
        print("Regression: Transaction_Cost ~ Relational_Complexity + Controls")
        
        # Calculate correlation
        r_value, p_value = self.pearson_correlation(relational_complexity, total_costs)
        
        print(f"Relational Complexity ~ Total Costs:")
        print(f"  r = {r_value:.3f}, p = {p_value:.6f}")
        
        if r_value > 0.5 and p_value < 0.001:
            print("âœ“ STRONG evidence for Complexity-Tax Hypothesis")
        elif r_value > 0.3 and p_value < 0.05:
            print("âœ“ MODERATE evidence for Complexity-Tax Hypothesis")
        else:
            print("âœ— WEAK evidence for Complexity-Tax Hypothesis")
        
        # Calculate economic impact
        low_complexity = np.percentile(relational_complexity, 25)  # 25th percentile
        high_complexity = np.percentile(relational_complexity, 75)  # 75th percentile
        
        low_cost = base_transaction_cost + complexity_multiplier * low_complexity
        high_cost = base_transaction_cost + complexity_multiplier * high_complexity
        complexity_tax = high_cost - low_cost
        
        print(f"\nEconomic Impact Analysis:")
        print(f"  Low-complexity orgs (RC = {low_complexity:.1f}): ${low_cost:,.0f}")
        print(f"  High-complexity orgs (RC = {high_complexity:.1f}): ${high_cost:,.0f}")
        print(f"  COMPLEXITY TAX: ${complexity_tax:,.0f} per organization")
        
        results = {
            'correlation_r': r_value,
            'correlation_p': p_value,
            'complexity_tax': complexity_tax,
            'cost_range': (np.min(total_costs), np.max(total_costs))
        }
        
        self.protocol_results['complexity_tax'] = results
        return results
    
    def run_comprehensive_validation(self):
        """
        Run all experimental protocols and provide integrated validation
        """
        print("AEP MORALITY - COMPREHENSIVE EXPERIMENTAL VALIDATION")
        print("=" * 60)
        
        # Run all protocols
        fmri_results = self.simulate_fmri_data()
        eeg_results = self.simulate_eeg_courage() 
        org_results = self.simulate_organizational_complexity_tax()
        
        # Integrated validation assessment
        print("\n" + "=" * 60)
        print("EXPERIMENTAL VALIDATION SUMMARY")
        print("=" * 60)
        
        # H1: Compression-Signature Hypothesis
        h1_support = sum(1 for metric in fmri_results.values() 
                        if metric['significant'] and metric['effect_size'] > 0.8)
        h1_strength = h1_support / len(fmri_results)
        
        # H2: Complexity-Tax Hypothesis  
        h2_support = org_results['correlation_r'] > 0.4 and org_results['correlation_p'] < 0.01
        
        # H3: Alignment-Dynamics Hypothesis
        h3_support = eeg_results['xi_synchrony_correlation'] > 0.3
        
        print("HYPOTHESIS VALIDATION RESULTS:")
        print(f"  H1 (Compression-Signature): {h1_strength:.1%} supported "
              f"({h1_support}/{len(fmri_results)} metrics)")
        print(f"  H2 (Complexity-Tax): {'SUPPORTED' if h2_support else 'NOT SUPPORTED'} "
              f"(r = {org_results['correlation_r']:.3f})")
        print(f"  H3 (Alignment-Dynamics): {'SUPPORTED' if h3_support else 'NOT SUPPORTED'} "
              f"(r = {eeg_results['xi_synchrony_correlation']:.3f})")
        
        # Overall validation score
        validation_score = (
            h1_strength + 
            (1.0 if h2_support else 0.0) + 
            (1.0 if h3_support else 0.0)
        ) / 3.0
        
        print(f"\nOVERALL VALIDATION SCORE: {validation_score:.1%}")
        
        if validation_score >= 0.7:
            print("ðŸŽ¯ AEP MORALITY EXPERIMENTALLY VALIDATED!")
            print("All three core hypotheses show substantial support")
        elif validation_score >= 0.5:
            print("âš ï¸  PARTIAL EXPERIMENTAL SUPPORT")
            print("Some hypotheses supported, others need refinement")
        else:
            print("âŒ LIMITED EXPERIMENTAL SUPPORT")
            print("Theory requires significant refinement")
        
        return {
            'h1_strength': h1_strength,
            'h2_support': h2_support,
            'h3_support': h3_support,
            'validation_score': validation_score
        }
    
    # Statistical helper functions (numpy-only)
    
    def t_cdf_approx(self, t, df):
        """Approximate t-distribution CDF"""
        if df > 30:
            return 0.5 * (1 + math.erf(t / math.sqrt(2)))
        else:
            z = t * (1 - 1/(4*df)) / math.sqrt(1 + t**2/(2*df))
            return 0.5 * (1 + math.erf(z / math.sqrt(2)))
    
    def logistic_predict(self, features, coefficients):
        """Logistic regression prediction"""
        linear_combination = (
            coefficients['intercept'] +
            coefficients['lpp_amplitude'] * features['lpp_amplitude'] +
            coefficients['synchrony'] * features['synchrony'] + 
            coefficients['alignment_confidence'] * features['alignment_confidence']
        )
        return 1 / (1 + np.exp(-linear_combination))
    
    def pearson_correlation(self, x, y):
        """Calculate Pearson correlation coefficient"""
        x, y = np.array(x), np.array(y)
        n = len(x)
        
        mean_x, mean_y = np.mean(x), np.mean(y)
        cov = np.sum((x - mean_x) * (y - mean_y))
        var_x = np.sum((x - mean_x)**2)
        var_y = np.sum((y - mean_y)**2)
        
        if var_x == 0 or var_y == 0:
            return 0.0, 1.0
            
        r = cov / np.sqrt(var_x * var_y)
        
        # Simplified p-value
        if n > 2:
            t_stat = r * math.sqrt((n-2) / (1 - r**2)) if abs(r) < 1 else float('inf')
            p_value = 2 * (1 - self.t_cdf_approx(abs(t_stat), n-2))
        else:
            p_value = 1.0
            
        return r, p_value

def demonstrate_protocol_analysis():
    """
    Demonstrate detailed analysis for each experimental protocol
    """
    print("DETAILED PROTOCOL ANALYSIS")
    print("=" * 60)
    
    protocols = ExperimentalProtocols()
    
    # Detailed fMRI analysis
    print("\n" + "=" * 40)
    print("fMRI PROTOCOL - COMPRESSION SIGNATURES")
    print("=" * 40)
    
    fmri_results = protocols.simulate_fmri_data(n_subjects=40, n_trials=120)
    
    print("\nCompression Signature Analysis:")
    for metric, results in fmri_results.items():
        effect_direction = "DECREASE" if "dimensionality" in metric or "predictive" in metric else "INCREASE"
        print(f"  {metric:>25}: {effect_direction} during virtue "
              f"(d = {results['effect_size']:.2f})")
    
    # Detailed EEG analysis
    print("\n" + "=" * 40)
    print("EEG PROTOCOL - ALIGNMENT DYNAMICS")
    print("=" * 40)
    
    eeg_results = protocols.simulate_eeg_courage(n_subjects=30)
    
    print(f"\nCourage Neural Correlates:")
    print(f"  LPP Amplitude: â†‘ during courageous action")
    print(f"  Frontal-Parietal Synchrony: â†‘ during courageous action") 
    print(f"  Alignment Confidence (Î¾): correlates with neural synchrony")
    
    # Detailed organizational analysis
    print("\n" + "=" * 40)
    print("ORGANIZATIONAL PROTOCOL - COMPLEXITY TAX")
    print("=" * 40)
    
    org_results = protocols.simulate_organizational_complexity_tax(n_organizations=100)
    
    print(f"\nComplexity Tax Breakdown:")
    print(f"  Relational Complexity Range: 30-100 bits")
    print(f"  Annual Costs Range: ${org_results['cost_range'][0]:,.0f}-${org_results['cost_range'][1]:,.0f}")
    print(f"  Average Complexity Tax: ${org_results['complexity_tax']:,.0f} per org")

if __name__ == "__main__":
    # Run comprehensive validation
    protocols = ExperimentalProtocols()
    validation_results = protocols.run_comprehensive_validation()
    
    # Show detailed analysis
    demonstrate_protocol_analysis()
    
    print("\n" + "=" * 60)
    print("EXPERIMENTAL FRAMEWORK READY")
    print("=" * 60)
    print("âœ“ Three falsifiable hypotheses implemented")
    print("âœ“ Neuroscientific protocols (fMRI, EEG) ready")
    print("âœ“ Sociological/economic protocols ready") 
    print("âœ“ Statistical validation framework complete")
    print("âœ“ All protocols produce quantitative, testable predictions")
